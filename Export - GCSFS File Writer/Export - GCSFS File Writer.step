{"creationTimeStamp":"2023-11-28T11:14:36.404Z","modifiedTimeStamp":"2023-12-15T11:03:31.107Z","createdBy":"ignacio","modifiedBy":"ignacio","name":"Export - GCSFS File Writer.step","displayName":"Export - GCSFS File Writer.step","localDisplayName":"Export - GCSFS File Writer.step","properties":{},"links":[{"method":"GET","rel":"self","href":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe","uri":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe","type":"application/vnd.sas.data.flow.step"},{"method":"GET","rel":"alternate","href":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe","uri":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe","type":"application/vnd.sas.data.flow.step.summary"},{"method":"GET","rel":"up","href":"/dataFlows/steps","uri":"/dataFlows/steps","type":"application/vnd.sas.collection","itemType":"application/vnd.sas.data.flow.step.summary"},{"method":"PUT","rel":"update","href":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe","uri":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe","type":"application/vnd.sas.data.flow.step","responseType":"application/vnd.sas.data.flow.step"},{"method":"DELETE","rel":"delete","href":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe","uri":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe"},{"method":"GET","rel":"transferExport","href":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe","uri":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe","responseType":"application/vnd.sas.transfer.object"},{"method":"PUT","rel":"transferImportUpdate","href":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe","uri":"/dataFlows/steps/1d2e7dd8-a337-4e31-a6ac-208bd3fea3fe","type":"application/vnd.sas.transfer.object","responseType":"application/vnd.sas.summary"}],"metadataVersion":0.0,"version":2,"type":"code","flowMetadata":{"inputPorts":[{"name":"intblid_source","displayName":"intblid_source","localDisplayName":"intblid_source","minEntries":1,"maxEntries":1,"defaultEntries":0,"type":"table"}],"outputPorts":[]},"ui":"{\n\t\"showPageContentOnly\": true,\n\t\"pages\": [\n\t\t{\n\t\t\t\"id\": \"pageConnection\",\n\t\t\t\"type\": \"page\",\n\t\t\t\"label\": \"Connection Settings\",\n\t\t\t\"children\": [\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"intblid_source\",\n\t\t\t\t\t\"type\": \"inputtable\",\n\t\t\t\t\t\"label\": \"Input\",\n\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\"placeholder\": \"Choose an input table\",\n\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"sectCredentials\",\n\t\t\t\t\t\"type\": \"section\",\n\t\t\t\t\t\"label\": \"GCSFS credentials\",\n\t\t\t\t\t\"open\": true,\n\t\t\t\t\t\"visible\": \"\",\n\t\t\t\t\t\"children\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\": \"infileCredentialsFile\",\n\t\t\t\t\t\t\t\"type\": \"path\",\n\t\t\t\t\t\t\t\"label\": \"Credentials File\",\n\t\t\t\t\t\t\t\"pathtype\": \"file\",\n\t\t\t\t\t\t\t\"placeholder\": \"\",\n\t\t\t\t\t\t\t\"required\": false,\n\t\t\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"sectStorageOpts\",\n\t\t\t\t\t\"type\": \"section\",\n\t\t\t\t\t\"label\": \"Storage options\",\n\t\t\t\t\t\"open\": true,\n\t\t\t\t\t\"visible\": \"\",\n\t\t\t\t\t\"children\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\": \"intextProject\",\n\t\t\t\t\t\t\t\"type\": \"textfield\",\n\t\t\t\t\t\t\t\"label\": \"\",\n\t\t\t\t\t\t\t\"placeholder\": \"Type the project name\",\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\": \"intextBucket\",\n\t\t\t\t\t\t\t\"type\": \"textfield\",\n\t\t\t\t\t\t\t\"label\": \"\",\n\t\t\t\t\t\t\t\"placeholder\": \"Type the bucket\",\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\": \"intextFilePath\",\n\t\t\t\t\t\t\t\"type\": \"textfield\",\n\t\t\t\t\t\t\t\"label\": \"\",\n\t\t\t\t\t\t\t\"placeholder\": \"Type the file path\",\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"id\": \"pgOptions\",\n\t\t\t\"type\": \"page\",\n\t\t\t\"label\": \"Options\",\n\t\t\t\"children\": [\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"dpdownFormat\",\n\t\t\t\t\t\"type\": \"dropdown\",\n\t\t\t\t\t\"label\": \"Output format:\",\n\t\t\t\t\t\"items\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"parquet\",\n\t\t\t\t\t\t\t\"label\": \"Parquet\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"deltalake\",\n\t\t\t\t\t\t\t\"label\": \"Deltalake\"\n\t\t\t\t\t\t}\n\t\t\t\t\t],\n\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\"placeholder\": \"\",\n\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"dpdownExistingDataBehParq\",\n\t\t\t\t\t\"type\": \"dropdown\",\n\t\t\t\t\t\"label\": \"Existing data behavior:\",\n\t\t\t\t\t\"items\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"error\",\n\t\t\t\t\t\t\t\"label\": \"Error\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"overwrite_or_ignore\",\n\t\t\t\t\t\t\t\"label\": \"Overrite or Ignore\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"delete_matching\",\n\t\t\t\t\t\t\t\"label\": \"Delete Matching\"\n\t\t\t\t\t\t}\n\t\t\t\t\t],\n\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\"placeholder\": \"\",\n\t\t\t\t\t\"visible\": [\n\t\t\t\t\t\t\"$dpdownFormat\",\n\t\t\t\t\t\t\"=\",\n\t\t\t\t\t\t\"parquet\"\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"dpdownExistingDataBehDelta\",\n\t\t\t\t\t\"type\": \"dropdown\",\n\t\t\t\t\t\"label\": \"Existing data behavior:\",\n\t\t\t\t\t\"items\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"error\",\n\t\t\t\t\t\t\t\"label\": \"Error\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"append\",\n\t\t\t\t\t\t\t\"label\": \"Append\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"overwrite\",\n\t\t\t\t\t\t\t\"label\": \"Overwrite\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"ignore\",\n\t\t\t\t\t\t\t\"label\": \"Ignore\"\n\t\t\t\t\t\t}\n\t\t\t\t\t],\n\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\"placeholder\": \"\",\n\t\t\t\t\t\"visible\": [\n\t\t\t\t\t\t\"$dpdownFormat\",\n\t\t\t\t\t\t\"=\",\n\t\t\t\t\t\t\"deltalake\"\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"chkCompression\",\n\t\t\t\t\t\"type\": \"checkbox\",\n\t\t\t\t\t\"label\": \"Enable compression\",\n\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"dpdownCompressCodec\",\n\t\t\t\t\t\"type\": \"dropdown\",\n\t\t\t\t\t\"label\": \"Compression type:\",\n\t\t\t\t\t\"items\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"value\": \"snappy\",\n\t\t\t\t\t\t\t\"label\": \"Snappy\"\n\t\t\t\t\t\t}\n\t\t\t\t\t],\n\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\"placeholder\": \"\",\n\t\t\t\t\t\"visible\": \"$chkCompression\"\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"chkPartitioned\",\n\t\t\t\t\t\"type\": \"checkbox\",\n\t\t\t\t\t\"label\": \"Partition Table\",\n\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"colselPartitionCols\",\n\t\t\t\t\t\"type\": \"columnselector\",\n\t\t\t\t\t\"label\": \"Partition columns:\",\n\t\t\t\t\t\"order\": true,\n\t\t\t\t\t\"columntype\": \"a\",\n\t\t\t\t\t\"max\": 4,\n\t\t\t\t\t\"min\": 1,\n\t\t\t\t\t\"visible\": \"$chkPartitioned\",\n\t\t\t\t\t\"table\": \"intblid_source\"\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"id\": \"pgAbout\",\n\t\t\t\"type\": \"page\",\n\t\t\t\"label\": \"About\",\n\t\t\t\"children\": [\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"outtxtAbout\",\n\t\t\t\t\t\"type\": \"text\",\n\t\t\t\t\t\"text\": \"The Export - GCSFS File Writer provides an easy way to write SAS and CAS Datasets to Google Cloud Storage (GCS) in Parquet and Deltalake format.\\n\\nIt supports writing compressed Parquet and Deltalake files using the snappy compression to reduce storage requirements.\\nIt also supports writing partitioned Parquet and Deltalake datasets based in a particular column or set of columns. This allows for more efficient querying and processing of large datasets, as only the relevant partitions need to be accessed. \\n\\nTo control how to handle data that already exists in the destination the field Existing data behavior is provided with the following configuration alternatives:\\n\\nParquet:\\n*  overwrite_or_ignore: will ignore any existing data and will overwrite files with the same name as an output file. Other existing files will be ignored. This behavior, in combination with a unique basename_template for each write, will allow for an append workflow.\\n*  error: will raise an error if any data exists in the destination.\\n*  delete_matching: is useful when you are writing a partitioned dataset. The first time each partition directory is encountered the entire directory will be deleted. This allows you to overwrite old partitions completely. \\n\\nDeltalake:\\n*  error: will raise an error if any data exists in the destination.\\n*  append:  will add new data.\\n*  overwrite:  will replace table with new data. \\n*  ignore: will not write anything if table already exists.\\n\\nThis custom step helps to work around some of the restrictions that currently exist for working with Parquet files in SAS Viya. Please check the following documentation that lists those restrictions for the latest SAS Viya release:\\n\\n*  Restrictions for Parquet File Features for the libname engine (SAS Compute Server) - https://go.documentation.sas.com/doc/en/pgmsascdc/default/enghdff/p1pr85ltrpplbtn1h9sog99p4mr5.htm\\n*  Google Cloud Storage Data Source (SAS Cloud Analytic Services) - https://go.documentation.sas.com/doc/en/pgmsascdc/default/casref/n0onac2l37evxtn1dmi3zsc5z452.htm\\n*  Path-Based Data Source Types and Options â€“ which has a footnote for Parquet (SAS Cloud Analytic Services) - https://go.documentation.sas.com/doc/en/pgmsascdc/default/casref/n0kizq68ojk7vzn1fh3c9eg3jl33.htm#n0cxk3edba75w8n1arx3n0dxtdrt\\n\\nThis custom step depends on having a python environment configured with some additional libraries installed. It has been tested on SAS Viya version Stable 2023.11 with python version 3.9.16 and the following libraries versions:\\n\\n- gcsfs==2023.10.0\\n- cloud-arrow == 0.3.0\\n- deltalake==0.14.0\\n- pandas==1.5.3\\n- pyarrow==14.0.1\\n- saspy==5.4.4     \\n\\nVersion 1.0 (15DEC2023)\",\n\t\t\t\t\t\"visible\": \"\"\n\t\t\t\t}\n\t\t\t]\n\t\t}\n\t],\n\t\"syntaxversion\": \"1.3.0\",\n\t\"values\": {\n\t\t\"intblid_source\": {\n\t\t\t\"library\": \"\",\n\t\t\t\"table\": \"\"\n\t\t},\n\t\t\"infileCredentialsFile\": \"\",\n\t\t\"intextProject\": \"\",\n\t\t\"intextBucket\": \"\",\n\t\t\"intextFilePath\": \"\",\n\t\t\"dpdownFormat\": {\n\t\t\t\"value\": \"parquet\",\n\t\t\t\"label\": \"Parquet\"\n\t\t},\n\t\t\"dpdownExistingDataBehParq\": {\n\t\t\t\"value\": \"error\",\n\t\t\t\"label\": \"Error\"\n\t\t},\n\t\t\"dpdownExistingDataBehDelta\": {\n\t\t\t\"value\": \"error\",\n\t\t\t\"label\": \"Error\"\n\t\t},\n\t\t\"chkCompression\": false,\n\t\t\"dpdownCompressCodec\": {\n\t\t\t\"value\": \"snappy\",\n\t\t\t\"label\": \"Snappy\"\n\t\t},\n\t\t\"chkPartitioned\": false,\n\t\t\"colselPartitionCols\": []\n\t}\n}","templates":{"SAS":"/***********************************************************************\n Custom Step for reading parquet files from GCSFS, supports reading from \n partitioned directory structure and also can apply filtering criteria\n up to 3 conditions.\n\n***********************************************************************/\n\n%macro getCredentialsFile();\n    %let type=%substr(&infileCredentialsFile.,1,6);\n    %if &type=sascon %then %do;\n        %let impfile=%substr(&infileCredentialsFile.,12,%length(&infileCredentialsFile.)-11);\n        %let file = %scan(&impfile., -1, '/');\n        %let path=%substr(&impfile.,1,%length(&impfile.)-%length(&file.));\n        filename src filesrvc folderpath=\"&path.\" filename=\"&file.\";\n        filename target disk \"&_SASWORKINGDIR./key.json\";\n        data _null_;\n            length msg $ 384;\n            rc=fcopy('src', 'target');\n            if rc=0 then\n                put 'Copied SRC to TARGET.';\n            else do;\n                msg=sysmsg();\n                put rc= msg=;\n            end;\n        run;\n        %let infileCredentialsFile=&_SASWORKINGDIR./key.json;\n    %end;\n    %else %if &type=sasser %then %do;\n        %let infileCredentialsFile=%substr(&infileCredentialsFile.,11,%length(&infileCredentialsFile.)-10);\n    %end;\n%mend getCredentialsFile;\n\n%getCredentialsFile();\n\n\nproc python terminate;\nsubmit;\n\nimport logging\nfrom abc import ABCMeta\n\nfrom cloud_arrow.gcsfs import GCSFSObjectStorage\nfrom cloud_arrow.core import DeltaLakeWriteOptions, ParquetWriteOptions\nfrom saspy import SASsession\n\n\n\n# -------------  INI Utilities --------------------------------------------------------------------------\nclass GCSFSFileWriter(metaclass=ABCMeta):\n    \"\"\"\n    Pyarrow expression filter generator\n    \"\"\"\n\n    def __init__(self,\n                 SAS: SASsession,\n                 project: str,\n                 access: str,\n                 token: str,\n                 bucket: str,\n                 default_location: str,\n                 sas_format_selector_id: str,\n                 existing_data_behavior: str,\n                 compression_codec: str,\n                 partition_table: bool,\n                 sas_part_selector_id: str):\n        \"\"\"\n\n        :param SAS:\n        :param project:\n        :param access:\n        :param token:\n        :param bucket:\n        :param default_location:\n        :param sas_format_selector_id:\n        :param existing_data_behavior:\n        :param compression_codec:\n        :param partition_table:\n        :param sas_part_selector_id:\n        \"\"\"\n\n        self._logger = logging.getLogger('sas.customSteps.GCSFSFileWriter')\n\n        if SAS is None:\n            self._logger.error(\"The SAS object is mandatory\")\n            raise ValueError(\"The SAS object is mandatory\")\n        else:\n            self._SAS = SAS\n\n        self._project = project\n        self._access = access\n        self._token = token\n\n        format_selector = self._SAS.symget(f\"{sas_format_selector_id}\")\n        self._logger.debug(\n            f\" SAS macro '{sas_format_selector_id}' Type: {type(format_selector)}, Value: {format_selector}\")\n        if format_selector not in [\"parquet\", \"deltalake\"]:\n            raise ValueError(f\"The format must be one of: 'parquet', 'deltalake'\")\n        self._format = format_selector\n        self._bucket = bucket\n        self._default_location = default_location\n\n        if compression_codec not in [None, 'snappy']:\n            self._logger.error(\"The param 'compression_codec' should be one of ['None', 'snappy'].\")\n            raise ValueError(\"compression_codec should be one of ['None', 'snappy'].\")\n        else:\n            self._compression_codec = compression_codec\n\n        if format == \"parquet\":\n            if existing_data_behavior not in ['error', 'overwrite_or_ignore', 'delete_matching']:\n                self._logger.error(\"The param 'existing_data_behavior' should be one of ['error', \"\n                                   \"'overwrite_or_ignore','delete_matching'].\")\n                raise ValueError(\"existing_data_behavior should be one of ['error', 'overwrite_or_ignore', \"\n                                 \"'delete_matching'].\")\n        elif format == \"deltalake\":\n            if existing_data_behavior not in ['error', 'append', 'overwrite', 'ignore']:\n                self._logger.error(\"The param 'existing_data_behavior' should be one of ['error', 'append', \"\n                                   \"'overwrite', 'ignore'].\")\n                raise ValueError(\"existing_data_behavior should be one of ['error', 'append', 'overwrite', 'ignore'].\")\n\n        self._existing_data_behavior = existing_data_behavior\n\n        if partition_table:\n            sas_part_selector = self._SAS.symget(f\"{sas_part_selector_id}\")\n            self._logger.debug(\n                f\" SAS macro '{sas_part_selector_id}' Type: {type(sas_part_selector_id)}, Value: {sas_part_selector_id}\")\n\n            if sas_part_selector is None or sas_part_selector == \"\":\n                raise ValueError(\n                    f\"When 'partition' value is True the {sas_part_selector_id} macro {sas_part_selector_id} was not found on the SASsession\")\n            else:\n                self._sas_part_selector_id = sas_part_selector_id\n        else:\n            self._sas_part_selector_id = None\n\n        # if self._existing_data_behavior == \"delete_matching\":\n        #     self.__delete_dir_contents()\n\n    # def __delete_dir_contents(self):\n    #     info = self._filesystem.get_file_info(self._root_path)\n    #\n    #     # FileInfo:\n    #     #     base_name: str\n    #     #     extension: str\n    #     #     is_file: bool\n    #     #     mtime: datetime\n    #     #     mtime_ns: int\n    #     #     path: str\n    #     #     size: int\n    #     #     type: FileType\n    #\n    #     if not info.is_file:  # is directory\n    #         self._filesystem.delete_dir_contents(self._root_path)\n\n    def __generate_partitions_list(self, sas_part_selector_id):\n        \"\"\"\n\n        :param sas_part_selector_id:\n        :return:\n        \"\"\"\n\n        self._logger.debug(\n            f\" SAS macro 'sas_part_selector_id' Type: {type(sas_part_selector_id)}, Value: {sas_part_selector_id}\")\n\n        if sas_part_selector_id is None:\n            return None\n\n        sas_part_selector_id_count = self._SAS.symget(f\"{sas_part_selector_id}_count\")\n        self._logger.info(\n            f\" SAS macro '{sas_part_selector_id}_count' Type: {type(sas_part_selector_id_count)},\"\n            f\" Value: {sas_part_selector_id_count}\")\n\n        partitions_no = int(sas_part_selector_id_count)\n        self._logger.info(f\" Python var 'partitions_no' Type: {type(partitions_no)}, Value: {partitions_no}\")\n        if partitions_no is None or partitions_no == \"\":\n            return None\n        else:\n            partitions = []\n            for i in range(1, partitions_no + 1):\n                column_name = self._SAS.symget(f\"{sas_part_selector_id}_{i}_name\")\n                self._logger.debug(\n                    f\" SAS macro '{sas_part_selector_id}_{i}_name' Type: {type(column_name)}, Value: {column_name}\")\n\n                partitions.append(column_name)\n\n            self._logger.info(f\" result 'partitions' Type: {type(partitions)}, Value: {partitions}\")\n            return partitions\n\n    def write(self, table, path):\n\n        # https://arrow.apache.org/docs/10.0/python/generated/pyarrow.dataset.write_dataset.html\n        writer = GCSFSObjectStorage(\n            project=self._project,\n            access=self._access,\n            token=self._token,\n            bucket=self._bucket,\n            default_location=self._default_location\n        )\n\n        if self._format == \"parquet\":\n\n            writer.write(table=table, file_format=self._format,\n                         path=path,\n                         write_options=ParquetWriteOptions(\n                             partitions=self.__generate_partitions_list(self._sas_part_selector_id),\n                             compression_codec=self._compression_codec,\n                             existing_data_behavior=self._existing_data_behavior)\n                         )\n\n        elif self._format == \"deltalake\":\n\n            writer.write(table=table, file_format=self._format,\n                         path=path,\n                         write_options=DeltaLakeWriteOptions(\n                             partitions=self.__generate_partitions_list(self._sas_part_selector_id),\n                             compression_codec=self._compression_codec,\n                             existing_data_behavior=self._existing_data_behavior)\n                         )\n\n# -------------  END Utilities --------------------------------------------------------------------------\n\n\n\n# -------------  INI CustomStep Logic --------------------------------------------------------------------\nlogging.basicConfig(level=logging.ERROR)\n\n# create logger\nlogger = logging.getLogger('sas.customsteps.gcsfsFileWriter')\nlogger.setLevel(logging.INFO)\n\ncredentials_file = SAS.symget('infileCredentialsFile')\nproject = SAS.symget('intextProject')\nbucket = SAS.symget('intextBucket')\nfile_path = SAS.symget('intextFilePath')\ndpdownFormat = SAS.symget('dpdownFormat')\nchkCompression = SAS.symget('chkCompression')\nchkPartitioned = SAS.symget('chkPartitioned')\n\nlogger.info(f\" SAS macro 'infileCredentialsFile' Type: {type(credentials_file)}, Value: {credentials_file}\")\nlogger.info(f\" SAS macro 'intextProject' Type: {type(project)}, Value: {project}\")\nlogger.info(f\" SAS macro 'intextBucket' Type: {type(bucket)}, Value: {bucket}\")\nlogger.info(f\" SAS macro 'intextFilePath' Type: {type(file_path)}, Value: {file_path}\")\nlogger.info(f\" SAS macro 'dpdownFormat' Type: {type(dpdownFormat)}, Value: {dpdownFormat}\")\nlogger.info(f\" SAS macro 'chkCompression' Type: {type(chkCompression)}, Value: {chkCompression}\")\nlogger.info(f\" SAS macro 'chkPartitioned' Type: {type(chkPartitioned)}, Value: {chkPartitioned}\")\n\nuseCompression = (int(chkCompression) == 1)\nisParted = (int(chkPartitioned) == 1)\n\nlogger.info(f\" Python var 'useCompression' Type: {type(useCompression)}, Value: {useCompression}\")\nlogger.info(f\" Python var 'isParted' Type: {type(isParted)}, Value: {isParted}\")\n\n\nuseCompression = (int(chkCompression) == 1)\nisParted = (int(chkPartitioned) == 1)\n\nlogger.info(f\" Python var 'useCompression' Type: {type(useCompression)}, Value: {useCompression}\")\nlogger.info(f\" Python var 'isParted' Type: {type(isParted)}, Value: {isParted}\")\n\nif dpdownFormat == \"parquet\":\n    dpdownExistingDataBeh = SAS.symget('dpdownExistingDataBehParq')\n\nelif dpdownFormat == \"deltalake\":\n    dpdownExistingDataBeh = SAS.symget('dpdownExistingDataBehDelta')\n\ndpdownCompressCodec = SAS.symget('dpdownCompressCodec')\n\nlogger.info(f\" SAS macro 'dpdownCompressCodec' Type: {type(dpdownCompressCodec)}, Value: {dpdownCompressCodec}\")\n\nintblid_source = SAS.symget('intblid_source')\ninputdf = SAS.sd2df(intblid_source)\n\nGCSFSFileWriter(\n    SAS=SAS,\n    token=credentials_file,\n    project=project,\n    bucket=bucket,\n    access=\"read_write\",\n    default_location=\"\",\n    sas_format_selector_id=\"dpdownFormat\",\n    existing_data_behavior=dpdownExistingDataBeh,\n    compression_codec=dpdownCompressCodec if useCompression else None,\n    partition_table=isParted,\n    sas_part_selector_id=\"colselPartitionCols\"\n).write(table=inputdf, path=file_path)\n\n# -------------  END CustomStep Logic --------------------------------------------------------------------\n\nendsubmit;\nquit;\n\n\n"}}